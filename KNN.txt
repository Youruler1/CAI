# -*- coding: utf-8 -*-
"""K_NN_Classifier_(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10hQ7qZiMywIJRqjvxi3IAYZOsWs9sCxQ
"""

import numpy as np
import pandas as pd

from sklearn.datasets import load_iris
iris=load_iris()

X=iris.data
Y=iris.target

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)

X_test.shape

#X_test[0,:].shape

X_train.shape

#X_test[0,:].reshape(1,-1).shape

"""indices = [1, 3, 4]  # We want the elements at index 1, 3, and 4

# Using .take to retrieve these elements
selected_elements = arr.take(indices)

Original array: [10 20 30 40 50]
Selected elements: [20 40 50]
"""

import heapq
import scipy
Y_label=[]
for i in range(len(X_test)):
    #print(X_test.shape)
    X=np.array(X_test[i,:]).reshape(1,-1)
    ary = scipy.spatial.distance.cdist(X_train, X, metric='euclidean')#X_train  (120, 4)    X(1,4)
    #print(ary.shape) # ary (120, 1)
    indx=heapq.nsmallest(5, range(len(ary)), ary.take)
    Y_neighbors=[]
    for k in range(len(indx)):
        Y_neighbors.append(Y_train[indx[k]])
    Y_label.append(max(set(Y_neighbors), key = Y_neighbors.count))

from sklearn import metrics
print(metrics.classification_report(Y_test,Y_label))
print(metrics.confusion_matrix(Y_test,Y_label))

#Fine-Tune K
test_mse=[]
n_neighbors=[i for i in range(3,10)]
for j in range(len(n_neighbors)):
    Y_label=[]
    for i in range(len(X_test)):
        X=np.array(X_test[i,:]).reshape(1,-1)
        ary = scipy.spatial.distance.cdist(X_train, X, metric='euclidean')
        indx=heapq.nsmallest(n_neighbors[j], range(len(ary)), ary.take)
        Y_neighbors=[]
        for k in range(len(indx)):
            Y_neighbors.append(Y_train[indx[k]])
        Y_label.append(max(set(Y_neighbors), key = Y_neighbors.count))
    test_mse.append(np.sqrt((metrics.mean_squared_error(Y_test,Y_label))))

import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))
plt.xlabel('K')
plt.ylabel('Root Mean Square Error')
plt.xlim([3,10])
plt.plot(test_mse,linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)



from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load a sample dataset
data = load_iris()
X, y = data.data, data.target

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Different values of k to test

}

# Create a KNN model
knn = KNeighborsClassifier()

# Set up GridSearchCV
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit GridSearchCV to find the best parameters
grid_search.fit(X_train, y_train)

# Print the best parameters and best score
print("Best parameters found:", grid_search.best_params_)
print("Best cross-validated accuracy:", grid_search.best_score_)